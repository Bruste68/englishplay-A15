import { useEffect, useState, useRef } from 'react';
import * as FileSystem from 'expo-file-system';
import { Audio } from 'expo-av';
import axios from 'axios';
import { WHISPER_URL } from '../lib/env';
import type { Message } from '../types';

console.log(' ìœ„ìŠ¤í¼ ì£¼ì†Œ0:', process.env.EXPO_PUBLIC_WHISPER_URL);

const WHISPER_API_URL = WHISPER_URL;

// âœ… ì„œë²„ ê¸°ë°˜ Whisper ì „ì‚¬ í•¨ìˆ˜ ì •ì˜
async function transcribeAudio(uri: string, lang = 'en'): Promise<string> {
  try {

    console.log('ğŸ¯ Whisper ì „ì†¡ URL:', `${WHISPER_API_URL}/transcribe`);
    console.log('ğŸ§ Whisper ì „ì†¡ íŒŒì¼ URI:', uri);

    const formData = new FormData();
    formData.append('file', {
      uri,
      type: 'audio/mp4',
      name: 'recording.m4a',
    } as any);
    formData.append('language', lang);

    const response = await axios.post(`${WHISPER_API_URL}/transcribe`, formData, {
      headers: { 'Content-Type': 'multipart/form-data' },
      timeout: 5000, // âœ… 5ì´ˆ ì•ˆì— ì‘ë‹µ ì—†ìœ¼ë©´ ì—ëŸ¬
    });
    console.log('âœ… Whisper ì‘ë‹µ ê²°ê³¼:', response.data);
    console.log('ğŸ¯ Whisper ìš”ì²­ ì£¼ì†Œ:', `${WHISPER_API_URL}/transcribe`);

    return response.data?.text || '';
  } catch (err) {
    console.error('âŒ [WHISPER API ERROR]', err);
    return "__NETWORK_ERROR__"; // âœ… ëª…ì‹œì  ì—ëŸ¬ í”Œë˜ê·¸ ë°˜í™˜
  }
}

let globalRecordingInstance: Audio.Recording | null = null;
export const isAutoRecordingInProgress = { current: false };

export function useVoice() {
  const [isRecording, setIsRecording] = useState(false);
  const [transcript, setTranscript] = useState('');
  const [error, setError] = useState<string | null>(null);
  const [audioUri, setAudioUri] = useState<string | null>(null);
  const dialogActiveRef = useRef(true); // ê¸°ë³¸ê°’ true
  const isStoppingRef = { current: false };

  // ì™¸ë¶€ì—ì„œ ì´ í•¨ìˆ˜ë¡œ ëŒ€í™” ìƒíƒœ ê°±ì‹ 
  const setDialogActive = (active: boolean) => {
    dialogActiveRef.current = active;
  };

  const silenceTimer = useRef<number | null>(null);
  const silenceCounter = useRef(0);
  const isAutoRecordingInProgress = useRef(false);
  const silenceThreshold = 4500;
  const vadIntervalRef = useRef<number | null>(null);
  const isProcessing = useRef(false);
  const [localMessages, setLocalMessages] = useState<Message[]>([]);

  const addMessage = (text: string, role: 'user' | 'ai', step?: number, meta?: any) => {
    const newMessage = {
      text,
      role,
      step,
      timestamp: Date.now(),
      ...(meta || {}),
    };
    setLocalMessages((prev) => [...prev, newMessage]);
  };

  const cleanupRecording = async (): Promise<void> => {
    if (!globalRecordingInstance) return;
    try {
      const rec = globalRecordingInstance;
      globalRecordingInstance = null;

      const status = await rec.getStatusAsync().catch(() => null);
      if (status?.isRecording) {
        await rec.stopAndUnloadAsync().catch(() => {});
      } else {
        await rec.stopAndUnloadAsync().catch(() => {});
      }
    } catch (err) {
      console.warn('[RECORDING CLEANUP WARN]', err);
    } finally {
      globalRecordingInstance = null;
    }
  };

  const _startRecording = async (): Promise<boolean> => {
    if (isProcessing.current) return false;
    isProcessing.current = true;

    try {
      await cleanupRecording();

      const { status, canAskAgain } = await Audio.requestPermissionsAsync();
      if (status !== 'granted') {
        if (!canAskAgain) {
          setError('ğŸ¤ ë§ˆì´í¬ ê¶Œí•œì´ í•„ìš”í•©ë‹ˆë‹¤. ì•± ì„¤ì •ì—ì„œ ë§ˆì´í¬ ê¶Œí•œì„ í—ˆìš©í•´ì£¼ì„¸ìš”.');
        } else {
          // íŒì—…ì€ ëœ° ìˆ˜ ìˆì§€ë§Œ, ê±°ë¶€ëœ ìƒíƒœ
          setError('ë§ˆì´í¬ ê¶Œí•œì´ ê±°ë¶€ë˜ì—ˆìŠµë‹ˆë‹¤.');
        }
        throw new Error('Permission denied');
      }

      await Audio.setAudioModeAsync({
        allowsRecordingIOS: true,
        playsInSilentModeIOS: true,
      });

      const { recording } = await Audio.Recording.createAsync(
        Audio.RecordingOptionsPresets.HIGH_QUALITY
      );

      globalRecordingInstance = recording;
      setIsRecording(true);
      startVADLoop();

      return true;
    } catch (err) {
      console.error('[START RECORDING ERROR]', err);
      return false;
    } finally {
      isProcessing.current = false;
    }
  };

  const stopRecording = async (): Promise<void> => {
    if (isStoppingRef.current) {
      console.log('â¸ï¸ [SKIP] stopRecording already running');
      return;
    }
    isStoppingRef.current = true;

    const rec = globalRecordingInstance;
    if (!rec) {
      console.log('â„¹ï¸ [STOP] No active recording');
      isStoppingRef.current = false;
      return;
    }
    globalRecordingInstance = null;

    try {
      stopVADLoop();

      // URI ë¨¼ì € í™•ë³´
      let uri: string | null = null;
      try {
        uri = rec.getURI?.() ?? null;
      } catch {}

      // ì•ˆì „í•œ unload
      await rec.stopAndUnloadAsync().catch((e) => {
        console.log('[STOP WARN]', e);
      });

      setIsRecording(false);
      setAudioUri(uri);

      if (uri) {
        console.log('ğŸ”ˆ [STOP] URI:', uri);
        try {
          const text = await transcribeAudio(uri, 'en');
          if (!dialogActiveRef.current) {
            console.log('â›” ëŒ€í™” ì¢…ë£Œ í›„ Whisper ì‘ë‹µ ë¬´ì‹œ');
            return;
          }
          if (!text || text.trim() === "") {
            setTranscript("__NO_SPEECH__");
          } else {
            setTranscript(text.trim());
          }
        } catch (e) {
          console.log('[WHISPER WARN]', e);
        }
      }
    } catch (err) {
      console.error('[STOP RECORDING ERROR]', err);
    } finally {
      isStoppingRef.current = false;
    }
  };

  // âœ… useVoice.tsì— ì¶”ê°€í•  ê²ƒ
  const stopAllRecordingLogic = () => {
    stopVADLoop();
    if (silenceTimer.current) {
      clearTimeout(silenceTimer.current as unknown as number);
      silenceTimer.current = null;
    }
    isAutoRecordingInProgress.current = false;
    dialogActiveRef.current = false;
  };

  const startAutoRecording = async (duration: number = 14000): Promise<void> => {
    if (isAutoRecordingInProgress.current) return;
    isAutoRecordingInProgress.current = true;

    try {
      const success = await _startRecording();
      if (!success) {
        isAutoRecordingInProgress.current = false;
        return;
      }

      if (duration) {
        setTimeout(async () => {
          await stopRecording();
          isAutoRecordingInProgress.current = false;
        }, duration);
      }
    } catch (err) {
      isAutoRecordingInProgress.current = false;
      console.error('[AUTO RECORDING ERROR]', err);
    }
  };

  const abortWhisper = () => {
    console.log('ğŸš« [WHISPER] ì„œë²„ ê¸°ë°˜ì´ë¯€ë¡œ abort ë¶ˆí•„ìš”');
  };

  const clearTranscript = () => {
    setTranscript('');
    setError(null);
  };

  const startVADLoop = () => {
    if (!globalRecordingInstance) return;

    vadIntervalRef.current = setInterval(async () => {
      const status = await globalRecordingInstance?.getStatusAsync();
      const volume = status?.metering;

      if (volume != null && volume < -55) {
        silenceCounter.current += 1;
        if (silenceCounter.current >= 20) { 
          stopRecording();
          silenceCounter.current = 0;
        }
      } else {
        silenceCounter.current = 0;
      }
    }, 200) as unknown as number;
  };

  const stopVADLoop = () => {
    if (vadIntervalRef.current) {
      clearInterval(vadIntervalRef.current as unknown as number);
      vadIntervalRef.current = null;
    }
    if (silenceTimer.current) {
      clearTimeout(silenceTimer.current as unknown as number);
      silenceTimer.current = null;
    }
  };

  useEffect(() => {
    return () => {
      stopVADLoop();
      cleanupRecording();
    };
  }, []);

  return {
    isRecording,
    transcript,
    audioUri,
    error,
    startAutoRecording,
    stopRecording,
    clearTranscript,
    addMessage,
    setLocalMessages, 
    startRecording: _startRecording,
    abortWhisper,
    setDialogActive,
    stopAllRecordingLogic,
  };
}
